{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2758ee-e892-4e0d-af96-57c06a067aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1861kB [00:00, 2722.34kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded succesfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Downloading the case study dataset if not already done\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm    \n",
    "\n",
    "url = 'https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/tender-details/docs/745b51da-4cfd-4ce9-86f6-ae0d6738f67a-CN/FoodEx2-CaseStudy2-Dataset_V1.xlsx'\n",
    "f = 'data/FoodEx2-CaseStudy2-Dataset_V1.xlsx'\n",
    "force = False\n",
    "if not os.path.exists(f) or force:\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(f, \"wb\") as handle:\n",
    "        for data in tqdm(response.iter_content(chunk_size=1024), unit=\"kB\"):\n",
    "            handle.write(data)\n",
    "    print(\"File downloaded succesfully\")\n",
    "else:\n",
    "    print(\"Skipping download, file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "205c2363-09fe-4131-89f9-404d0a829d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a hierarchy for the code RISKF04, skipping this model\n",
      "Could not find a hierarchy for the code F15, skipping this model\n",
      "Could not find a hierarchy for the code F14, skipping this model\n",
      "Training test datasets stored in data/datasets-training-test.pickle\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Extracting separate training and test datasets for all classification task\n",
    "1. baseterm (expo)\n",
    "2. facets\n",
    "3. F01, F02, etc\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "SEED = 44\n",
    "def main():\n",
    "    f_case = \"data/FoodEx2-CaseStudy2-Dataset_V1.xlsx\"\n",
    "    f_datasets = \"data/datasets-training-test.pickle\"\n",
    "    if not os.path.exists(f_case):\n",
    "        raise FileNotFoundError(\"Please run previous notebook to obtain the case dataset\")\n",
    "    force = False\n",
    "    if not os.path.exists(f_datasets) or force:\n",
    "        case_df = pd.read_excel(f_case)\n",
    "        # adding all term and facet categories\n",
    "        datasets = {\n",
    "            \"baseterm\":term_df(case_df),\n",
    "            \"facets\":facets_df(case_df)\n",
    "        }\n",
    "        # getting all facets categories found\n",
    "        fcats = datasets[\"facets\"].category.unique().tolist()\n",
    "        # iterating trough all facets categories and adding the associated training dataset\n",
    "        for fcat in fcats:\n",
    "            df = facet_df(case_df, fcat)\n",
    "            datasets[fcat] = df\n",
    "    \n",
    "        # splitting into training 90% and testing 10% of data and saving to disk\n",
    "        for experiment, df in datasets.items():\n",
    "            if df is not None:\n",
    "                shuffled = df.sample(frac=1, random_state = SEED)\n",
    "                limit = int(len(df)/5)\n",
    "                training = df.iloc[:, limit:]\n",
    "                test = df.iloc[:, :limit]\n",
    "            else:\n",
    "                training = None\n",
    "                test = None\n",
    "                \n",
    "            datasets[experiment] = {\"experiment\":experiment, \"training\":training, \"test\":test}\n",
    "        with open(f_datasets, \"wb\") as f:\n",
    "            pickle.dump(datasets, f)\n",
    "        print(f\"Training test datasets stored in {f_datasets}\")\n",
    "    else:\n",
    "        print(\"Training test datasets file already exist, skipping\")\n",
    "\n",
    "def term_df(df):\n",
    "    df = df.rename(columns={\"ENFOODNAME\":\"text\"})\n",
    "    df[\"hierarchy\"] = \"expo\"\n",
    "    df[\"category\"] = df[\"FACETS\"].str.split(\"#\").str[0]\n",
    "    df = (\n",
    "        df[[\"text\", \"hierarchy\", \"category\"]][pd.notna(df.text) & pd.notna(df.text)]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop = True)\n",
    "    \n",
    "    )\n",
    "    return df\n",
    "\n",
    "def facets_df(df):\n",
    "    df = df.rename(columns={\"ENFOODNAME\":\"text\"})\n",
    "    df[\"hierarchy\"] = \"facets\"\n",
    "    # extracting all facets\n",
    "    df[\"category\"] = (\n",
    "        df[\"FACETS\"]\n",
    "            .str.split(\"#\") #split term and facets\n",
    "            .str[1] #choose only facets\n",
    "            .str.split(\"$\") #split on each facet\n",
    "    )\n",
    "    # tranforming multiple facets into different rows\n",
    "    df = df[[\"text\", \"hierarchy\", \"category\"]][pd.notna(df.text) & pd.notna(df.text)].explode(\"category\")\n",
    "\n",
    "    # extracting the facet hierarchy from the facet detail\n",
    "    df[\"category\"] = df[\"category\"].str.split(\".\").str[0]\n",
    "\n",
    "    df = df[pd.notna(df.category)].drop_duplicates().reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def facet_df(df, fcat):\n",
    "    # gettin a translation from facets to respetive hierrarcy codes\n",
    "    fmap = {fcat:hcode for i, fcat, hcode in pd.read_pickle(\"data/attributes.pickle\")[[\"code\", \"name\"]].itertuples()}    \n",
    "    if fcat not in fmap:\n",
    "        print(f\"Could not find a hierarchy for the code {fcat}, skipping this model\") \n",
    "        return None\n",
    "    df = df.rename(columns={\"ENFOODNAME\":\"text\"})\n",
    "    # extracting all facets\n",
    "    df[\"category\"] = (\n",
    "        df[\"FACETS\"]\n",
    "            .str.split(\"#\") #split term and facets\n",
    "            .str[1] #choose only facets\n",
    "            .str.split(\"$\") #split on each facet\n",
    "    )\n",
    "    df[\"fcat\"] = df.category.str.split(\"\")\n",
    "    # tranforming multiple facets into different rows\n",
    "    df = df[[\"text\", \"category\"]][pd.notna(df.text) & pd.notna(df.text)].explode(\"category\")\n",
    "    # extracting the facet hierarchy from the facet detail\n",
    "    df[\"fcat\"] = df[\"category\"].str.split(\".\").str[0]\n",
    "    # limiting to the expected category\n",
    "    df = df[df.fcat == fcat]\n",
    "    df[\"category\"] = df[\"category\"].str.split(\".\").str[0]\n",
    "    df[\"hierarchy\"] = fmap[fcat]\n",
    "\n",
    "\n",
    "    df = df[pd.notna(df.category)][[\"text\", \"hierarchy\", \"category\"]].drop_duplicates().reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
