{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cff5aa3b-bdb5-4669-9d74-53b627b7e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "defining experiments\n",
    "'''\n",
    "SEED = 44\n",
    "def experiments():\n",
    "    experiments = [\n",
    "        {\"name\":\"sentence_similarity\",\n",
    "             \"strategy\": \"one_vs_all\",\n",
    "             \"model_type\":\"similarity\", \n",
    "             \"model\":\"sentence-transformers/all-MiniLM-L6-v2\"},\n",
    "        {\"name\":\"zero_shot_hypothesis\", \n",
    "             \"strategy\": \"expand_parents\", \n",
    "             \"model_type\":\"zero-shot-hypothesis\", \n",
    "             \"model\":\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\"},\n",
    "             #\"model\":\"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\"},#\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"},#\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\"},\n",
    "             #\"model\":\"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\"},#\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"},#\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\"},\n",
    "        {\"name\":\"zero_shot_simple\", \n",
    "             \"strategy\": \"expand_parents\", \n",
    "             \"model_type\":\"zero-shot\", \n",
    "             \"model\":\"facebook/bart-large-mnli\"},\n",
    "    ]\n",
    "    return experiments\n",
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b766c44a-ae52-4a83-89aa-74dd7504ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Doing prediction for zero_shot_hypothesis\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoritzLaurer/deberta-v3-large-zeroshot-v2.0 loaded\n",
      "... predicting facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The hierarchy facets has no question defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 226\u001b[0m\n\u001b[1;32m    223\u001b[0m   torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    224\u001b[0m   torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats()\n\u001b[0;32m--> 226\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 41\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],model_type, device)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... predicting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39mhierarchy_name\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m predicted_codes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhierarchy_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterms_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#saving results as csv file\u001b[39;00m\n\u001b[1;32m     43\u001b[0m res \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[25], line 75\u001b[0m, in \u001b[0;36mpredict_with_strategy\u001b[0;34m(hierarchy_name, training_df, testing_df, terms_df, strategy, n, model, model_type)\u001b[0m\n\u001b[1;32m     73\u001b[0m predicted \u001b[38;5;241m=\u001b[39m predict_through_parents(input_text, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m}, hierarchy_name, hierarchy_code, terms_df, \u001b[38;5;241m5\u001b[39m, model, model_type)\n\u001b[1;32m     74\u001b[0m retmap \u001b[38;5;241m=\u001b[39m {text:code \u001b[38;5;28;01mfor\u001b[39;00m text, code \u001b[38;5;129;01min\u001b[39;00m predicted}\n\u001b[0;32m---> 75\u001b[0m bests \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mretmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhierarchy_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m     76\u001b[0m best_codes \u001b[38;5;241m=\u001b[39m [retmap[text] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m bests]\n\u001b[1;32m     77\u001b[0m predicted_codes[i] \u001b[38;5;241m=\u001b[39m best_codes[\u001b[38;5;241m0\u001b[39m:n]\n",
      "Cell \u001b[0;32mIn[25], line 111\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(texts, categories, hierarchy_name, n, model, model_type)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-hypothesis\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(results)):\n\u001b[0;32m--> 111\u001b[0m         hypothesis_template \u001b[38;5;241m=\u001b[39m \u001b[43mget_hierarchy_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhierarchy_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(texts[i], categories, hypothesis_template\u001b[38;5;241m=\u001b[39mhypothesis_template, multi_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m         topn \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m v, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(((v, i) \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m])), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)][\u001b[38;5;241m0\u001b[39m:n]\n",
      "Cell \u001b[0;32mIn[25], line 209\u001b[0m, in \u001b[0;36mget_hierarchy_question\u001b[0;34m(hierarchy)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis contains the additive of type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe hierarchy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhierarchy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no question defined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The hierarchy facets has no question defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "force_cpu = False\n",
    "device = \"cuda\" if torch.cuda.is_available() and not force_cpu else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "def main():\n",
    "    # getting terms hierarchy\n",
    "    terms_df = get_terms_df()\n",
    "    # getting training and test dataset for each hierarchy\n",
    "    datasets = load_datasets()\n",
    "    # Iterating over defined experiments    \n",
    "    force = False\n",
    "    for exp in experiments():\n",
    "        # loading model\n",
    "        flush() #ensuring memory is freed before loading a model\n",
    "        exp_name = exp['name']\n",
    "        model_type =  exp[\"model_type\"]\n",
    "        strategy = exp[\"strategy\"]\n",
    "        model = None\n",
    "        # Iterating on datasets for each hierchy    \n",
    "        for ds in datasets.values():\n",
    "            # extracting data data for the current hierarchy\n",
    "            hierarchy_name = ds[\"hierarchy\"]\n",
    "            training_df = ds[\"training\"]\n",
    "            f_results = f\"data/result_{hierarchy_name}_{exp_name}.csv\"\n",
    "            if (not os.path.exists(f_results) or force) and training_df is not None :\n",
    "                #doing predictions\n",
    "                test_df = ds[\"test\"].head(500)\n",
    "                if model is None: \n",
    "                    print(f\"Doing prediction for {exp['name']}\")\n",
    "                    model = load_model(exp[\"model\"],model_type, device)\n",
    "                print(f\"... predicting { hierarchy_name }\")\n",
    "                predicted_codes = predict_with_strategy(hierarchy_name, training_df, test_df, terms_df, strategy, 10, model, model_type)\n",
    "                #saving results as csv file\n",
    "                res = test_df.copy()\n",
    "                res[\"experiment\"] = exp_name\n",
    "                res[\"hierarchy\"] = hierarchy_name\n",
    "                res[\"predicted_categories\"] = predicted_codes\n",
    "                res[\"in_top_one\"] = res.apply(lambda r: r[\"category\"] in r[\"predicted_categories\"][0:1], axis=1)\n",
    "                res[\"in_top_three\"] = res.apply(lambda r: r[\"category\"] in r[\"predicted_categories\"][0:3], axis=1)\n",
    "                res[\"in_top_ten\"] = res.apply(lambda r: r[\"category\"] in r[\"predicted_categories\"][0:10], axis=1)\n",
    "                res.to_csv(f_results)\n",
    "            \n",
    "    \n",
    "def predict_with_strategy(hierarchy_name, training_df, testing_df, terms_df, strategy, n, model, model_type):\n",
    "    if testing_df is None:\n",
    "       print(\"skipping hierarcy {hierarchy_name}\") \n",
    "    elif strategy == \"one_vs_all\":\n",
    "        hierarchy_codes = testing_df.hierarchy.unique().tolist()\n",
    "        assert(len(hierarchy_codes)==1)\n",
    "        hierarchy_code = hierarchy_codes[0]\n",
    "        text_map = get_hierarchy_map(terms_df, hierarchy_code)  \n",
    "        categories = [* text_map.keys()]\n",
    "        input_texts = testing_df.text.tolist()\n",
    "        predicted_texts = predict(inppredicted_codesut_texts, categories, hierarchy_name, n, model, model_type)\n",
    "        return [[text_map[text] for text in predicted] for predicted in predicted_texts]\n",
    "    elif strategy == \"expand_parents\":\n",
    "        hierarchy_codes = testing_df.hierarchy.unique().tolist()\n",
    "        assert(len(hierarchy_codes)==1)\n",
    "        hierarchy_code = hierarchy_codes[0]\n",
    "        input_texts = testing_df.text.tolist() \n",
    "        predicted_codes = [None for t in input_texts]\n",
    "        i = 0\n",
    "        for input_text in  tqdm(input_texts):\n",
    "            predicted = predict_through_parents(input_text, {\"root\":\"root\"}, hierarchy_name, hierarchy_code, terms_df, 5, model, model_type)\n",
    "            retmap = {text:code for text, code in predicted}\n",
    "            bests = predict([input_text], [*retmap.keys()], hierarchy_name, n, model, model_type)[0] \n",
    "            best_codes = [retmap[text] for text in bests]\n",
    "            predicted_codes[i] = best_codes[0:n]\n",
    "            i = i +1\n",
    "        return predicted_codes\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported strategy {strategy}\")\n",
    "\n",
    "def load_model(model, model_type, device):\n",
    "    print(\"loading model\") \n",
    "    if model_type == \"similarity\":\n",
    "        m =  SentenceTransformer(model, device = device)\n",
    "    elif model_type.startswith(\"zero-shot\"):\n",
    "        m = pipeline(\"zero-shot-classification\", model=model, device = device)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "    print(f\"{model} loaded\") \n",
    "    return m\n",
    "    \n",
    "def predict(texts, categories, hierarchy_name, n, model, model_type):\n",
    "    results = [*range(0,len(texts))]\n",
    "    if model_type == \"similarity\":\n",
    "        text_vecs =  model.encode(texts, convert_to_tensor = True)\n",
    "        cat_vecs = model.encode(categories, convert_to_tensor = True)\n",
    "        for i in range(0, len(results)):\n",
    "            similarities = [util.pytorch_cos_sim(text_vecs[i], cat_vec).item() for cat_vec in cat_vecs]                \n",
    "            topn = [categories[j] for sim, j in sorted(((sim, j) for j, sim in enumerate(similarities)), reverse=True)][0:n]\n",
    "            results[i] = topn\n",
    "    elif model_type == \"zero-shot\":\n",
    "        for i in range(0, len(results)):\n",
    "            output = model(texts[i], categories)\n",
    "            topn = [i for v, i in sorted(((v, i) for i, v in enumerate(output[\"scores\"])), reverse=True)][0:n]\n",
    "            results[i] = [output['labels'][i] for i in topn]\n",
    "    elif model_type == \"zero-shot-hypothesis\":\n",
    "        for i in range(0, len(results)):\n",
    "            hypothesis_template = get_hierarchy_question(hierarchy_name)\n",
    "            output = model(texts[i], categories, hypothesis_template=hypothesis_template, multi_label=False)\n",
    "            topn = [i for v, i in sorted(((v, i) for i, v in enumerate(output[\"scores\"])), reverse=True)][0:n]\n",
    "            results[i] = [output['labels'][i] for i in topn]\n",
    "    else:\n",
    "        raise ValueError(f\"the model type {model_type} is not supported\")\n",
    "    return results\n",
    "\n",
    "    \n",
    "def predict_through_parents(input_text, parents, hierarchy_name, hierarchy_code, terms_df, n, model, model_type):\n",
    "    candidates = get_children_map({*parents.values()}, terms_df, hierarchy_code)\n",
    "    if len(candidates)==0:\n",
    "        return [*parents.items()]\n",
    "    cand_text = [*candidates.keys()]\n",
    "    bests = predict([input_text], [d for d,c in candidates.items()], hierarchy_name, n, model, model_type)[0]\n",
    "    best_codes = {text: candidates[text] for text in bests}\n",
    "\n",
    "    rest = predict_through_parents(input_text, best_codes, hierarchy_name, hierarchy_code, terms_df, n, model, model_type)\n",
    "    ret = [(text, code) for text, code in [*best_codes.items(), *rest]]\n",
    "    return ret\n",
    "    \n",
    "def get_hierarchy_map(terms_df, hierarchy_code):\n",
    "    #terms_df columns: termCode \tparentCode \thierarchyCode \tstatus\n",
    "    children = terms_df[(\n",
    "            (terms_df.hierarchyCode == hierarchy_code) & \n",
    "            (terms_df.status == \"APPROVED\" ) \n",
    "        )][[\"termExtendedName\", \"termCode\"]]\n",
    "    return {k:v for k, v in children.values}\n",
    "\n",
    "def get_children_map(parent_codes, terms_df, hierarchy_code):\n",
    "    #terms_df columns: termCode \tparentCode \thierarchyCode \tstatus\n",
    "    children = terms_df[(\n",
    "            (terms_df.hierarchyCode == hierarchy_code) & \n",
    "            (terms_df.status == \"APPROVED\" ) & \n",
    "            (terms_df.parentCode.isin(parent_codes))\n",
    "        )][[\"termExtendedName\", \"termCode\"]]\n",
    "    return {k:v for k, v in children.values}\n",
    "\n",
    "def get_hierarchy_question(hierarchy):\n",
    "    if hierarchy == \"baseterm\":\t\n",
    "        return \"This text describes a {}\"\n",
    "    elif hierarchy == \"F02\":\t#part\tPart-nature\tThis facet describes the nature of the food item or the part of plant or animal it represents.\n",
    "        return \"This is obtained from {}\"\n",
    "    elif hierarchy == \"F01\":    #source: Source\tThis facet describes the plant, animal, other organism or other source from which a raw primary commodity \n",
    "        return \"This is mainly obtained from {}\"\n",
    "    elif hierarchy == \"F27\":   #racsource\tSource-commodities\tThis facet describes the RPC from which an ingredient or derivative has been obtained.\n",
    "        return \"This is derivated from {}\"\n",
    "    elif hierarchy == \"F28\":    #process\tProcess\tThis facet allows recording different characteristics of the food: preservation treatments a food item underwent\n",
    "        return \"This is processed by {}\"\n",
    "    elif hierarchy == \"F04\":    #ingred\tIngredient\tThis facet collects ingredients and/or flavour note.\n",
    "        return \"This contains {}\"\n",
    "    elif hierarchy == \"F06\":\t    #medium\tSurrounding-medium\tThis facet is intended for food packed in any container, together with any additional (    fluid) medium.\n",
    "        return \"This is sell surrounded by {}\"\n",
    "    elif hierarchy == \"F08\":\t    #sweet\tSweetening-agent\tThis facet allows providing information on the added ingredient(s) used to impart sweetness to a food item.\n",
    "        return \"This can be sweeteded with {}\"\n",
    "    elif hierarchy == \"F09\":\t    #fort\tFortification-agent\tThis facet allows providing information on the added ingredient(s) used to fortify a food item.\n",
    "        return \"This can be fortified with {}\"\n",
    "    elif hierarchy == \"F10\":\t#qual\tQualitative-info\tThis facet provides some principal claims related to important nutrients-ingredients, like fat, sugar etc.\n",
    "        return \"When eated provides {}\"\n",
    "    elif hierarchy == \"F17\":\t#cookext\tExtent-of-cooking\tThis facet describes the intensity of heat treatment having been applied to a food item‚Äù.\n",
    "        return \"This is be cooked by {}\"\n",
    "    elif hierarchy == \"F26\":    #\tgen\tGeneric-term\tThis facet allows recording whether the food list code was chosen because of lack of information on the food item.\n",
    "        return \"This description is ambiguous because {}\"\n",
    "    elif hierarchy == \"F21\":    #prod\tProduction-method\tThe facet production method describes the method used to produce the food.\n",
    "        return \"This text is ambiguous because {}\"\n",
    "    elif hierarchy == \"F18\":    #packformat\tPackaging-format\tThis facet is used for packaged food and allows recording the container or wrapping form.\n",
    "        return \"This is sell in a {}\"\n",
    "    elif hierarchy == \"F19\":\t#packmat\tPackaging-material\tThis facet is used for packaged food and allows recording the material constituting the packaging containing.\n",
    "        return \"This is sell in a package made of {}\"\n",
    "    elif hierarchy == \"F03\":     #\tstate\tPhysical-state\tThis facet describes the form (physical aspect) of the food as reported by the consumer .\n",
    "        return \"This seems like a {}\"\n",
    "    elif hierarchy == \"F07\":     #fat\tFat-content\tThis is a facet with numerical descriptors, to allow providing the fat content (as percentage w/w) of a food item.\n",
    "        return \"This contains a fat level of {}\"\n",
    "    elif hierarchy == \"F11\":    #alcohol\tAlcohol-content\tThis is a facet containing information to allow providing the alcohol (ethanol) content (as percentage v/v) of a food item.\n",
    "        return \"This contains an alcohol level of {}\"\n",
    "    elif hierarchy == \"F12\":\t#dough\tDough-Mass\tThis facet is proposed to provide information on the original dough-mass, for bakery products.\n",
    "        return \"This contains a dough of {}\"\n",
    "    elif hierarchy == \"F20\":\t#partcon\tPart-consumed-analysed\tthis facet allows specifying in which form the food item was analysed or consumed.\n",
    "        return \"This is evaluated by analyzing its {}\"\n",
    "    elif hierarchy == \"F22\":    #place\tPreparation-production-place\tThis facet allows recording the place where the food was prepared for consumption.\n",
    "        return \"This prepared in a {}\"\n",
    "    elif hierarchy == \"F23\":    #targcon\tTarget-consumer\tThis facet allows recording different consumer classes intended as target for the food item.\n",
    "        return \"This is eated by {}\"\n",
    "    elif hierarchy == \"F24\":    #use\tIntended-use\tThis facet allows recording the intended use of a food item, in particular with respect to further treatment expected (or not expected) before consumption.\n",
    "        return \"This can gone through {}\"\n",
    "    elif hierarchy == \"F25\":\t#riskingred\tRisky-Ingredient\tThis facet (of specific interest in the microbiological domain) allows recording the presence of microbiologically high-risk ingredients.\n",
    "        return \"This is made with a dangerous {}\"\n",
    "    elif hierarchy == \"F29\":\t#fpurpose\tPurpose-of-raising\tThis facet allows recording the purpose of farming, keeping or breeding (e.g. milk production, egg production).\n",
    "        return \"This is farmed for {}\"\n",
    "    elif hierarchy == \"F30\":\t#replev\tReproductive-level\tThis facet allows recording classes of animals from the point of view of reproduction.\n",
    "        return \"This animal can reproduce by {}\"\n",
    "    elif hierarchy == \"F31\":    #\tanimage\tAnimal-age-class\tThis facet allows recording the classes of the animal used in legislation or in the practice, based on age or development stage.\n",
    "        return \"This animal age is {}\"\n",
    "    elif hierarchy == \"F32\":    #\tgender\tGender\tThis facet allows recording the status of an animal or animal group, with respect to sex.\n",
    "        return \"This animal gender is {}\"\n",
    "    elif hierarchy == \"F33\":\t    #legis\tLegislative-classes\tThis facet allows recording the food additives classes as reported in the legislation in order.\n",
    "        return \"This contains the additive of type {}\"\n",
    "    else:\n",
    "        raise ValueError(f\"The hierarchy {hierarchy} has no question defined\")\n",
    "\n",
    "def get_terms_df():\n",
    "    df = pd.read_pickle(\"data/terms.pickle\")\n",
    "    return df[[\"termCode\", \"termExtendedName\", \"parentCode\", \"hierarchyCode\", \"status\"]]\n",
    "\n",
    "def load_datasets():\n",
    "    f_datasets = \"data/datasets-training-test.pickle\"\n",
    "    with open(f_datasets, \"rb\") as f:\n",
    "        datasets = pickle.load(f)\n",
    "    return datasets\n",
    "\n",
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34490ab-13c4-40e2-8061-33a9539a7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "\n",
    "a = torch.randn(10, 1)\n",
    "b = torch.randn(1, 10) # different row number, for the fun\n",
    "print(a)\n",
    "print(b)\n",
    "# Given that cos_sim(u, v) = dot(u, v) / (norm(u) * norm(v))\n",
    "#                          = dot(u / norm(u), v / norm(v))\n",
    "# We fist normalize the rows, before computing their dot products via transposition:\n",
    "a_norm = a / a.norm(dim=1)[:, None]\n",
    "b_norm = b / b.norm(dim=1)[:, None]\n",
    "res = torch.mm(a_norm, b_norm.transpose(0,1))\n",
    "print(res)\n",
    "#  0.9978 -0.9986 -0.9985\n",
    "# -0.8629  0.9172  0.9172\n",
    "\n",
    "# -------\n",
    "# Let's verify with numpy/scipy if our computations are correct:\n",
    "a_n = a.numpy()\n",
    "b_n = b.numpy()\n",
    "res_n = np.zeros((2, 3))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        # cos_sim(u, v) = 1 - cos_dist(u, v)\n",
    "        res_n[i, j] = 1 - spatial.distance.cosine(a_n[i], b_n[j])\n",
    "print(res_n)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
