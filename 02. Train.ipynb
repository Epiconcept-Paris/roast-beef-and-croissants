{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e9fa22-d9c5-4dc6-86eb-a2e9d81162f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fod/github/epi-mapper-efsa/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42009/42009 [00:03<00:00, 11918.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14112/14112 [00:01<00:00, 11202.52 examples/s]\n",
      "/tmp/ipykernel_850505/44564322.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14112\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3939' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3939/3939 16:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Accuracy Balanced</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.155817</td>\n",
       "      <td>0.942311</td>\n",
       "      <td>0.942319</td>\n",
       "      <td>0.942290</td>\n",
       "      <td>0.942319</td>\n",
       "      <td>0.942347</td>\n",
       "      <td>0.942290</td>\n",
       "      <td>0.942319</td>\n",
       "      <td>0.942319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.156961</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>0.950172</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>0.950987</td>\n",
       "      <td>0.950172</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>0.950326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.167324</td>\n",
       "      <td>0.953007</td>\n",
       "      <td>0.953019</td>\n",
       "      <td>0.952952</td>\n",
       "      <td>0.953019</td>\n",
       "      <td>0.953163</td>\n",
       "      <td>0.952952</td>\n",
       "      <td>0.953019</td>\n",
       "      <td>0.953019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics:  {'f1_macro': 0.9423109917874593, 'f1_micro': 0.9423185941043084, 'accuracy_balanced': np.float64(0.9422897766254084), 'accuracy': 0.9423185941043084, 'precision_macro': 0.9423473542515224, 'recall_macro': 0.9422897766254084, 'precision_micro': 0.9423185941043084, 'recall_micro': 0.9423185941043084}\n",
      "Detailed metrics:  {'entailment': {'precision': 0.9403381305016069, 'recall': 0.9456231558240832, 'f1-score': 0.9429732380552053, 'support': 7117.0}, 'not_entailment': {'precision': 0.9443565780014378, 'recall': 0.9389563974267334, 'f1-score': 0.9416487455197132, 'support': 6995.0}, 'accuracy': 0.9423185941043084, 'macro avg': {'precision': 0.9423473542515224, 'recall': 0.9422897766254084, 'f1-score': 0.9423109917874593, 'support': 14112.0}, 'weighted avg': {'precision': 0.942329984261621, 'recall': 0.9423185941043084, 'f1-score': 0.942316716989037, 'support': 14112.0}} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850505/44564322.py:154: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics:  {'f1_macro': 0.9502923452239042, 'f1_micro': 0.9503259637188208, 'accuracy_balanced': np.float64(0.9501721507052097), 'accuracy': 0.9503259637188208, 'precision_macro': 0.9509866883998913, 'recall_macro': 0.9501721507052097, 'precision_micro': 0.9503259637188208, 'recall_micro': 0.9503259637188208}\n",
      "Detailed metrics:  {'entailment': {'precision': 0.9357511545775604, 'recall': 0.9679640297878319, 'f1-score': 0.9515850542164515, 'support': 7117.0}, 'not_entailment': {'precision': 0.9662222222222222, 'recall': 0.9323802716225875, 'f1-score': 0.9489996362313569, 'support': 6995.0}, 'accuracy': 0.9503259637188208, 'macro avg': {'precision': 0.9509866883998913, 'recall': 0.9501721507052097, 'f1-score': 0.9502923452239042, 'support': 14112.0}, 'weighted avg': {'precision': 0.9508549753098741, 'recall': 0.9503259637188208, 'f1-score': 0.9503035208543669, 'support': 14112.0}} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850505/44564322.py:154: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate metrics:  {'f1_macro': 0.9530067614619318, 'f1_micro': 0.9530187074829932, 'accuracy_balanced': np.float64(0.9529520925794263), 'accuracy': 0.9530187074829932, 'precision_macro': 0.9531630474451088, 'recall_macro': 0.9529520925794263, 'precision_micro': 0.9530187074829932, 'recall_micro': 0.9530187074829932}\n",
      "Detailed metrics:  {'entailment': {'precision': 0.9469529085872577, 'recall': 0.9606575804411971, 'f1-score': 0.9537560159029086, 'support': 7117.0}, 'not_entailment': {'precision': 0.95937318630296, 'recall': 0.9452466047176554, 'f1-score': 0.9522575070209548, 'support': 6995.0}, 'accuracy': 0.9530187074829932, 'macro avg': {'precision': 0.9531630474451088, 'recall': 0.9529520925794263, 'f1-score': 0.9530067614619318, 'support': 14112.0}, 'weighted avg': {'precision': 0.9531093600201755, 'recall': 0.9530187074829932, 'f1-score': 0.9530132388600183, 'support': 14112.0}} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850505/44564322.py:154: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from accelerate.utils import release_memory\n",
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import datasets\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "def main():\n",
    "    model_name=\"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\"\n",
    "    mode = \"trained-with-parents\" #\"retrained\"\n",
    "    force_cpu = False\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not force_cpu else \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "    max_length = 512\n",
    "    label2id = {\"entailment\": 0, \"not_entailment\": 1}  #{\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "    id2label = {0: \"entailment\", 1: \"not_entailment\"}  #{0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, model_max_length=max_length)  # model_max_length=512\n",
    "    label_text_unique = list(label2id.keys())\n",
    "    #model = pipeline(\"zero-shot-classification\", model=model_name, device = device)\n",
    "    if device == \"cuda\":\n",
    "        # free memory\n",
    "        flush()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, label2id=label2id, id2label=id2label\n",
    "    ).to(device)\n",
    "    \n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples[\"text\"], examples[\"hypothesis\"], truncation=True)\n",
    "\n",
    "\n",
    "    training, test = get_training_formatted(mode = mode)\n",
    "\n",
    "    encoded_dataset_train = datasets.Dataset.from_pandas(training).map(tokenize_func, batched=True)\n",
    "    print(len(encoded_dataset_train))\n",
    "    # testing during training loop on aggregated testset:\n",
    "    encoded_dataset_test = datasets.Dataset.from_pandas(test).map(tokenize_func, batched=True)\n",
    "    print(len(encoded_dataset_test))\n",
    "\n",
    "    # remove columns the library does not expect\n",
    "    encoded_dataset_train = encoded_dataset_train.remove_columns([\"hypothesis\", \"text\"])\n",
    "    encoded_dataset_test = encoded_dataset_test.remove_columns([\"hypothesis\", \"text\"])\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    run_name = f\"{model_name.split('/')[-1]}-zeroshot-retrained-{now}\"\n",
    "    training_directory = f'data/train/{run_name}'\n",
    "    fp16_bool = True if torch.cuda.is_available() else False\n",
    "    if \"mDeBERTa\" in model_name: fp16_bool = False  # mDeBERTa does not support FP16 yet\n",
    "\n",
    "    # https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments\n",
    "    eval_batch = 64 if \"large\" in model_name else 64*2\n",
    "    per_device_train_batch_size = 8 if \"large\" in model_name else 32\n",
    "    gradient_accumulation_steps = 4 if \"large\" in model_name else 1\n",
    "\n",
    "  \n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=training_directory,\n",
    "        logging_dir=f'{training_directory}/logs',\n",
    "        #deepspeed=\"ds_config_zero3.json\",  # if using deepspeed\n",
    "        lr_scheduler_type= \"linear\",\n",
    "        group_by_length=False,  # can increase speed with dynamic padding, by grouping similar length texts https://huggingface.co/transformers/main_classes/trainer.html\n",
    "        learning_rate=9e-6, #if \"large\" in model_name else 2e-5,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,  # (!adapt/halve batch size accordingly). accumulates gradients over X steps, only then backward/update. decreases memory usage, but also slightly speed\n",
    "        #eval_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        #max_steps=400,\n",
    "        #warmup_steps=0,  # 1000,\n",
    "        warmup_ratio=0.06,  #0.1, 0.06\n",
    "        weight_decay=0.01,  #0.1,\n",
    "        fp16=fp16_bool,   # ! only makes sense at batch-size > 8. loads two copies of model weights, which creates overhead. https://huggingface.co/transformers/performance.html?#fp16\n",
    "        fp16_full_eval=fp16_bool,\n",
    "        eval_strategy=\"epoch\",\n",
    "        seed=SEED,\n",
    "        #load_best_model_at_end=True,\n",
    "        #metric_for_best_model=\"accuracy\",\n",
    "        #eval_steps=300,  # evaluate after n steps if evaluation_strategy!='steps'. defaults to logging_steps\n",
    "        save_strategy=\"no\",  # options: \"no\"/\"steps\"/\"epoch\"\n",
    "        #save_steps=1_000_000,  # Number of updates steps before two checkpoint saves.\n",
    "        save_total_limit=1,  # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir\n",
    "        #logging_strategy=\"epoch\",\n",
    "        report_to=\"all\",  # \"all\"\n",
    "        run_name=run_name,\n",
    "        #push_to_hub=True,  # does not seem to work if save_strategy=\"no\"\n",
    "        #hub_model_id=hub_model_id,\n",
    "        #hub_token=config.HF_ACCESS_TOKEN,\n",
    "        #hub_strategy=\"end\",\n",
    "        #hub_private_repo=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        #model_init=model_init,\n",
    "        tokenizer=tokenizer,\n",
    "        args=train_args,\n",
    "        train_dataset=encoded_dataset_train,  #.shard(index=1, num_shards=200),  # https://huggingface.co/docs/datasets/processing.html#sharding-the-dataset-shard\n",
    "        eval_dataset=encoded_dataset_test,  #.shard(index=1, num_shards=20),\n",
    "        compute_metrics=lambda x: compute_metrics_standard(x, label_text_alphabetical=label_text_unique)  #compute_metrics,\n",
    "        #data_collator=data_collator,  # for weighted sampling per dataset; for dynamic padding probably not necessary because done by default  https://huggingface.co/course/chapter3/3?fw=pt\n",
    "    )\n",
    "    print(\"Training\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"Trained\")\n",
    "    finally:\n",
    "        if device == \"cuda\":\n",
    "            flush()\n",
    "            release_memory(model)\n",
    "\n",
    "    model_path = f'data/{model_name.split(\"/\")[-1]}-zeroshot-{mode}'\n",
    "\n",
    "\n",
    "    trainer.save_model(output_dir=model_path)\n",
    "\n",
    "\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "def compute_metrics_standard(eval_pred, label_text_alphabetical=None):\n",
    "    labels = eval_pred.label_ids\n",
    "    pred_logits = eval_pred.predictions\n",
    "    preds_max = np.argmax(pred_logits, axis=1)  # argmax on each row (axis=1) in the tensor\n",
    "\n",
    "    # metrics\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds_max, average='macro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, preds_max, average='micro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    acc_balanced = balanced_accuracy_score(labels, preds_max)\n",
    "    acc_not_balanced = accuracy_score(labels, preds_max)\n",
    "\n",
    "    metrics = {'f1_macro': f1_macro,\n",
    "            'f1_micro': f1_micro,\n",
    "            'accuracy_balanced': acc_balanced,\n",
    "            'accuracy': acc_not_balanced,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'precision_micro': precision_micro,\n",
    "            'recall_micro': recall_micro,\n",
    "            #'label_gold_raw': labels,\n",
    "            #'label_predicted_raw': preds_max\n",
    "            }\n",
    "    print(\"Aggregate metrics: \", {key: metrics[key] for key in metrics if key not in [\"label_gold_raw\", \"label_predicted_raw\"]} )  # print metrics but without label lists\n",
    "    print(\"Detailed metrics: \", classification_report(\n",
    "        labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),\n",
    "        target_names=label_text_alphabetical, sample_weight=None,\n",
    "        digits=2, output_dict=True, zero_division='warn'),\n",
    "    \"\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_training_formatted(mode):\n",
    "    with open(\"data/datasets-training-test.pickle\", \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    \n",
    "    training = d[\"baseterm\"][\"training\"]\n",
    "    codes =  [*training.category]\n",
    "    random.seed(SEED)\n",
    "    def other_codes(code):\n",
    "        return [*{*random.sample(codes, 1)} - {code}]\n",
    "    training[\"other_code\"] = training.apply(lambda r: other_codes(r[\"category\"]), axis = 1)\n",
    "    isa = training[[\"text\", \"hierarchy\", \"category\"]]\n",
    "    isa[\"labels\"] = 0\n",
    "    isnot =  training.explode(\"other_code\")[[\"text\", \"hierarchy\", \"other_code\"]].rename(columns = {\"other_code\":\"category\"})\n",
    "    isnot[\"labels\"] = 1\n",
    "    training = pd.concat([isa, isnot], ignore_index = True).sort_values([\"text\", \"labels\"])\n",
    "    terms = pd.read_pickle(\"data/terms.pickle\").set_index([\"hierarchyCode\", \"termCode\"])\n",
    "    joined = training.join(terms, on=[\"hierarchy\", \"category\"], how='inner', lsuffix='', rsuffix='', sort=False, validate=\"many_to_one\")\n",
    "    #assert len(training) == len(joined), f\"{len(training)} != {len(joined)}\"\n",
    "    joined = joined.reset_index()\n",
    "    limit = int(len(joined)/3)\n",
    "    joined = joined.sample(frac=1, random_state=SEED)\n",
    "    train = joined.iloc[limit:3*limit, :]\n",
    "    test = joined.iloc[:limit, :]\n",
    "    dfs = [train, test]\n",
    "    if mode == \"trained-with-parents\":\n",
    "        hCode = d[\"baseterm\"][\"training\"].hierarchy.unique().tolist()\n",
    "        assert len(hCode) == 1\n",
    "        hCode = hCode[0]\n",
    "        terms = pd.read_pickle(\"data/terms.pickle\")\n",
    "        parents =  {a:b for a,b in terms[terms.hierarchyCode == hCode][[\"termCode\", \"parentCode\"]].values}\n",
    "        descs = {a:b for a,b in terms[terms.hierarchyCode == hCode][[\"termCode\", \"termExtendedName\"]].values}\n",
    "        for i in range(0, len(dfs)):\n",
    "            current = {(text, code):(desc, label) for code,text,desc, label in dfs[i][[\"category\", \"text\", \"termExtendedName\", \"labels\"]].values}\n",
    "            labels = [0, 1] #start with 0 positive and then 1 negative\n",
    "            for lab in labels:\n",
    "                l = 0\n",
    "                while l < len(current):\n",
    "                    l = len(current)\n",
    "                    toadd = {(text, parents[code]): (descs[parents[code]], label) for (text, code), (desc, label) in current.items() if parents[code] != \"root\" and label == lab and (text, parents[code]) not in current } \n",
    "                    #print (lab, len(train), len(toadd))\n",
    "                    current = {**current, **toadd} \n",
    "            dfs[i] =  pd.DataFrame([{\"text\":text,\"termExtendedName\":desc, \"labels\":label } for (text, code),(desc, label) in current.items()]).drop_duplicates()\n",
    "            dfs[i] = dfs[i].rename(columns = {\"termExtendedName\": \"hypothesis\"})\n",
    "            dfs[i][\"hypothesis\"] = \"This food is a: \"+ dfs[i][\"hypothesis\"].map(str)\n",
    "            dfs[i][\"task_name\"] = \"food classification\"\n",
    "            # balancing classes\n",
    "    \n",
    "\n",
    "    cols = [\"text\", \"hypothesis\", \"labels\", \"task_name\"]\n",
    "    #limiyting class umbalance\n",
    "    tr = dfs[0].drop_duplicates().groupby([\"hypothesis\", \"labels\"]).head(40)[cols]\n",
    "    te = dfs[1].drop_duplicates().groupby([\"hypothesis\", \"labels\"]).head(10)[cols]\n",
    "    tr.to_csv(f\"data/training-{mode}.csv\")\n",
    "    te.to_csv(f\"data/validate-{mode}.csv\")\n",
    "    return tr,te \n",
    "   \n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4ccc10d-82a9-4954-a7da-ba5655b393a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>labels</th>\n",
       "      <th>task_name</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Know anyone for hire, Callie?\"</td>\n",
       "      <td>Callie, know anyone that's for hire?</td>\n",
       "      <td>0</td>\n",
       "      <td>mnli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The First Word in Protective &amp; Marine Coatings...</td>\n",
       "      <td>This text is about: protective coatings</td>\n",
       "      <td>0</td>\n",
       "      <td>mixtral_small_zeroshot</td>\n",
       "      <td>protective coatings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was in 1904 he and I worked together ”the A...</td>\n",
       "      <td>I met him for the first time in 1915.</td>\n",
       "      <td>1</td>\n",
       "      <td>mnli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mike Pence is a conservative Republican.</td>\n",
       "      <td>Mike Pence . He served as the chairman of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>fevernli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Management of Human Capital</td>\n",
       "      <td>There is no way to account for human capital.</td>\n",
       "      <td>1</td>\n",
       "      <td>mnli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018728</th>\n",
       "      <td>Since 1931, when gambling was officially legal...</td>\n",
       "      <td>Gambling is seen as a blight on Nevada and is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>mnli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018729</th>\n",
       "      <td>I have no doubt that several entries were part...</td>\n",
       "      <td>I have doubts that good examples existed.</td>\n",
       "      <td>1</td>\n",
       "      <td>mnli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018730</th>\n",
       "      <td>As a result, it is important to determine the ...</td>\n",
       "      <td>Heating water helps people.</td>\n",
       "      <td>1</td>\n",
       "      <td>wanli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018731</th>\n",
       "      <td>The river forms a natural line between the nor...</td>\n",
       "      <td>The north and south divide the city into secti...</td>\n",
       "      <td>0</td>\n",
       "      <td>mnli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018732</th>\n",
       "      <td>Selena Gomez is not an actress.</td>\n",
       "      <td>Selena Marie Gomez ( [ səˈliːnə_məˈɹiː_ˈɡoʊmɛz...</td>\n",
       "      <td>1</td>\n",
       "      <td>fevernli</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1018733 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "0                           Know anyone for hire, Callie?\"   \n",
       "1        The First Word in Protective & Marine Coatings...   \n",
       "2        It was in 1904 he and I worked together ”the A...   \n",
       "3                 Mike Pence is a conservative Republican.   \n",
       "4                              Management of Human Capital   \n",
       "...                                                    ...   \n",
       "1018728  Since 1931, when gambling was officially legal...   \n",
       "1018729  I have no doubt that several entries were part...   \n",
       "1018730  As a result, it is important to determine the ...   \n",
       "1018731  The river forms a natural line between the nor...   \n",
       "1018732                    Selena Gomez is not an actress.   \n",
       "\n",
       "                                                hypothesis  labels  \\\n",
       "0                     Callie, know anyone that's for hire?       0   \n",
       "1                  This text is about: protective coatings       0   \n",
       "2                    I met him for the first time in 1915.       1   \n",
       "3        Mike Pence . He served as the chairman of the ...       0   \n",
       "4            There is no way to account for human capital.       1   \n",
       "...                                                    ...     ...   \n",
       "1018728  Gambling is seen as a blight on Nevada and is ...       1   \n",
       "1018729          I have doubts that good examples existed.       1   \n",
       "1018730                        Heating water helps people.       1   \n",
       "1018731  The north and south divide the city into secti...       0   \n",
       "1018732  Selena Marie Gomez ( [ səˈliːnə_məˈɹiː_ˈɡoʊmɛz...       1   \n",
       "\n",
       "                      task_name           label_text  \n",
       "0                          mnli                 None  \n",
       "1        mixtral_small_zeroshot  protective coatings  \n",
       "2                          mnli                 None  \n",
       "3                      fevernli                 None  \n",
       "4                          mnli                 None  \n",
       "...                         ...                  ...  \n",
       "1018728                    mnli                 None  \n",
       "1018729                    mnli                 None  \n",
       "1018730                   wanli                 None  \n",
       "1018731                    mnli                 None  \n",
       "1018732                fevernli                 None  \n",
       "\n",
       "[1018733 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#ds = load_dataset(\"MoritzLaurer/dataset_train_nli\")[\"train\"]\n",
    "#ds = None\n",
    "ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8516d9b9-0253-4166-ad3d-890cd02ff77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hierarchy</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6703</th>\n",
       "      <td>Pork, shoulder with rind,reheated n.s.,canned,...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A0EYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6704</th>\n",
       "      <td>Pork, shoulder with rind,reheated n.s.,chilled...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A01RG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6705</th>\n",
       "      <td>Pork, shoulder with rind,roasted/baked in oven...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A01RG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6706</th>\n",
       "      <td>Pork, shoulder with rind,roasted/baked in oven...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A01RG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>Pork, shoulder with rind,stewed n.s.,chilled/r...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A01RG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33511</th>\n",
       "      <td>ZZZ-OUD VEGETABLES FOR NASI/BAMI/ETC.-FACETS_D...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A00ZQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33512</th>\n",
       "      <td>ZZZ-OUD VEGETABLES FOR NASI/BAMI/ETC.-FACETS_D...</td>\n",
       "      <td>expo</td>\n",
       "      <td>A00ZQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33513</th>\n",
       "      <td>Βaby food, cereal cream in a jar</td>\n",
       "      <td>expo</td>\n",
       "      <td>A03RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33514</th>\n",
       "      <td>Βroccoli</td>\n",
       "      <td>expo</td>\n",
       "      <td>A00FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33515</th>\n",
       "      <td>Τuna, canned with oil</td>\n",
       "      <td>expo</td>\n",
       "      <td>A0FBT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26813 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text hierarchy category\n",
       "6703   Pork, shoulder with rind,reheated n.s.,canned,...      expo    A0EYQ\n",
       "6704   Pork, shoulder with rind,reheated n.s.,chilled...      expo    A01RG\n",
       "6705   Pork, shoulder with rind,roasted/baked in oven...      expo    A01RG\n",
       "6706   Pork, shoulder with rind,roasted/baked in oven...      expo    A01RG\n",
       "6707   Pork, shoulder with rind,stewed n.s.,chilled/r...      expo    A01RG\n",
       "...                                                  ...       ...      ...\n",
       "33511  ZZZ-OUD VEGETABLES FOR NASI/BAMI/ETC.-FACETS_D...      expo    A00ZQ\n",
       "33512  ZZZ-OUD VEGETABLES FOR NASI/BAMI/ETC.-FACETS_D...      expo    A00ZQ\n",
       "33513                   Βaby food, cereal cream in a jar      expo    A03RJ\n",
       "33514                                           Βroccoli      expo    A00FN\n",
       "33515                              Τuna, canned with oil      expo    A0FBT\n",
       "\n",
       "[26813 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "def do():\n",
    "    terms = pd.read_pickle(\"data/terms.pickle\")\n",
    "    with open(\"data/datasets-training-test.pickle\", \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    hCode = d[\"baseterm\"][\"training\"].hierarchy.unique().tolist()\n",
    "    assert len(hCode) == 1\n",
    "    hCode = hCode[0]\n",
    "    training = d[\"baseterm\"][\"training\"]\n",
    "    \n",
    "\n",
    "    \n",
    "do()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
